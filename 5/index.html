<!DOCTYPE html>
<html>

<head>
    <title>My Portfolio</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        /* From https://css-tricks.com/almanac/properties/i/image-rendering/
        */
        img {
            image-rendering: auto;
            image-rendering: crisp-edges;
            image-rendering: pixelated;

            /* Safari seems to support, but seems deprecated and does the same thing as the others. */
            image-rendering: -webkit-optimize-contrast;
        }
    </style>
</head>

<body>

    <h1>Project 5: Diffusion Models</h1>

        <h2>Part A: Using Diffusion Models</h2>

            <h3>The DeepFloyd Model</h3>

                <p>Part A of this project relies on the DeepFloyd IF diffusion model, which is available on Hugging Face. The first stage of DeepFloyd IF takes a text prompt and outputs a 64x64 color image; the second stage upscales the image to 256x256.</p>

                <p>We call both stages of the model on three text prompts, as seen below. We vary the number of inference steps of each stage of the model; increasing the number of inference steps increases the required computation.</p>

                <span>20</span>
                <span>5</span>
                <span>40</span>

                <p>(I used a random seed of 18,000.)</p>

                <p>For <code>num_inference_steps=20</code> (the recommended value), the first stage generates a reasonable 64x64 image for each of the three prompts, and the second stage does a great job of upscaling. The output images appear slightly more high-contrast than real life; for example, the neon-green portion of the rocket is slightly unrealistic.</p>

                <p>For <code>num_inference_steps=5</code>, the 64x64 image for "a rocket ship" is nonsensical. Furthermore, the second stage does a poor job of upscaling the 64x64 image for "a man wearing a hat".
                </p>

                <p>For <code>num_inference_steps=40</code>, the first and second stages perform well, though their tendency to generate high-contrast images is increased.</p>


            <h3>1.1 The Forward Process</h3>

                <p>A diffusion model works by iteratively removing noise from a noisy image until it arrives at a "clean" output image. The model starts with an image of pure noise or with a noisy version of a clean image. In the latter case, the "forward process" of a diffusion model adds noise to the clean image.</p>

                <p>Given a clean image <code>x_0</code> and a timestep <code>t</code>, we compute the noisy image <code>x_t</code> as a linear combination of <code>x_0</code> and <code>epsilon</code>, where <code>epsilon</code> is a standard-normal image (in other words, an image of pure noise). For greater values of <code>t</code>, the contribution of <code>epsilon</code> is increased.</p>

                <p>We compute <code>x_t</code> for <code>t = [250, 500, 750]</code>. Our input <code>x_0</code> is a clean image of the Campanile.</p>

                <span>x_0</span>

                <span>x_t</span>

            <h3>1.2 Classical Denoising</h3>

                <p>A simple way to remove noise from an image is to blur it. We apply a Gaussian blur filter (with a kernel size of 13) to each <code>x_t</code>.</p>

                <span>denoised</span>
            
            <h3>1.3 One-step Denoising</h3>

                <p>A better way to remove noise from an image is to pass it into a denoising UNet model. DeepFloyd provides a trained denoising UNet.</p>

                <span>denoised</span>

            
            <h3>1.4 Iterative Denoising</h3>

                <p>The UNet performs worse on highly noisy inputs, which makes sense. But a diffusion model can denoise an input with any amount of noise. In particular, DeepFloyd can denoise an image with any amount of noise from <code>t=0</code> (no noise) to <code>t=1000</code> (entirely noise).</p>

                <p>For a noisy image <code>x_t</code>, DeepFloyd provides a noise estimate <code>err</code> and a scalar constant <code>alpha_cumprod_t</code> that can be used to obtain <code>x_{t-1}</code>. Over hundreds of iterations, we can transform <code>x_t</code> into a clean image <code>x_0</code>.</p>

                <p>(To compute the estimated noise of <code>x_t</code>, DeepFloyd must receive an embedded text prompt that describes the desired <code>x_0</code>. By default, we supply the text prompt "a high quality photo".)</p>

                <p>Iterating hundreds of times is costly. We define a sequence of "strided timesteps" in which each strided timestep corresponds to 30 canonical timesteps. In particular, we have <code>st[0]=990, st[1]=960, ..., st[33]=0</code>. We use a modified denoising formula to iterate over strided timesteps instead of canonical timesteps; in each iteration, we compute <code>x_{t-30}</code> instead of <code>x_{t-1}</code>.</p>

                <p>We show the iterative denoising of a noisy image <code>x_690</code>.</p>

                <span>progression</span>
                <span>result comparison</span>

            <h3>1.5 Diffusion Model Sampling</h3>

                <p>We sample an image from our denoising process by passing in a pure-noise image with timestep <code>t=990</code>. (We also pass in the default text prompt.)</p>

                <span>samples</span>

            <h3>1.6 Diffusion Model Sampling</h3>

                <p>We improve the quality of our sampled images with "Classifier-Free Guidance". In addition to the default text prompt ("a high quality photo"), we pass in the null prompt ("") to DeepFloyd. For a given <code>x_t</code>, each of our prompts provides a noise estimate. The former prompt is a "condition prompt" that provides our conditioned estimate <code>err_c</code>; the latter prompt is an "unconditioned prompt" provides our unconditioned estimate <code>err_u</code>.</p>

                <p>For some value <code>gamma &gt; 1</code>, we compute <code>err = err_u + gamma * (err_c - err_u)</code>. Note that <code>err</code> is essentially a weighted average in which <code>err_c</code> receives highly positive weight and <code>err_u</code> receives negative weight; informally, <code>err</code> is a more extreme version of <code>err_c</code>. We use <code>err</code> in our subsequent computation of <code>x_{t-30}.</code>

                <p>Now we sample images from our CFG-enhanced denoising process. We use the value <code>gamma = 7</code>.</p>

                <span>samples</span>

            <h3>1.7 Image-to-Image Translation</h3>

                <p>By providing the noisy version of a clean image to our denoising process, we obtain an output image that resembles the clean input with some random changes. For large <code>t</code> (e.g. <code>st[1]=960</code>), the output will have very little connection to the input; for small <code>t</code> (e.g. <code>st[20]=330</code>), the output will closely resemble the input.</p>

                <p>We use two real-life images as our clean inputs. For each image, we add noise to it according to a varying initial timestep and pass it into the denoising process.</p>

                <span>plushies</span>
                <span>view</span>
            
            <h3>1.7.1 More Images</h3>

                <p>We also perform image-to-image translations with web images and drawn images.</p>


            <h3>1.7.2 Inpainting</h3>

                <p>For our image of the Campanile and our two web images, we define the "masks" below.</p>

                <span>masks</span>
                
                <p>We can modify the denoising process so that it produces outputs with changes in the masked areas only. Then we execute the denoising process (with initial timestep <code>st[0]=990</code>, in order to maximize the changes in the masked areas).</p>

                <span>inpaintings - provide multiple?</span>

            <h3>1.7.3 Text-Conditioned Image-to-Image Translation</h3>
                
                <p>Observe that performing image-to-image translation with a high initial timestep yields a highly random output. We can reduce this randomness by replacing our generic condition prompt with a more specific condition prompt. (The unconditioned prompt stays the same.)</p>

                <span>results</span>

            <h3>1.8 Visual Anagrams</h3>

                <p>In this section and section 1.9, we produce visually interesting results by providing our denoising process with multiple condition prompts.</p>

                <p>A "visual anagram" depicts different things when viewed right-side-up and upside-down. To generate a visual anagram, we need a condition prompt for the right-side-up view and a condition prompt for the upside-down view.</p>

                <p>We execute our denoising process with a high initial timestep and an input of pure noise. For each (strided) iteration of the denoising process, we get a noise estimate <code>err_1</code> for the current <code>x_t</code> with our right-side-up prompt and a noise estimate <code>err_2</code> with our upside-down prompt. The weighted average <code>err = w * err_1 + (1 - w) * err_2</code> (for <code>0 &lt; w &lt; 1</code>) is used to compute <code>x_{t-30}</code>.</p> (Computing <code>err_2</code> requires us to flip <code>x_t</code> beforehand.)</p>

            <h3>1.9 Hybrid Images</h3>

                <p>A "hybrid image" depicts different things when viewed close-up and far-away. The close-up view relies on high-frequency features of the image, and the far-away view relies on low-frequency features. To generate a hybrid image, we need a condition prompt for the high-frequency features and a condition prompt for the low-frequency features.</p>

                <p></p>

        <h2>Part B: Diffusion Models From Scratch</h2>

                

 </body>
 </html>