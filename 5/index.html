<!DOCTYPE html>
<html>

<head>
    <title>My Portfolio</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        
        img {
            /* From https://css-tricks.com/almanac/properties/i/image-rendering/ */
            image-rendering: auto;
            image-rendering: crisp-edges;
            image-rendering: pixelated;
            image-rendering: -webkit-optimize-contrast;

            /* Center all images */
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        .caption {
            font-size: 12px;
            text-align: center;
        }
    </style>
</head>

<body>

    <h1>Project 5: Diffusion Models</h1>

        <h2>Part A: Using Diffusion Models</h2>

                <p>Part A of this project relies on the DeepFloyd IF diffusion model, which is available on Hugging Face. The first stage of DeepFloyd IF takes a text prompt and outputs a 64x64 color image; the second stage upscales the image to 256x256.</p>

                <p>We call both stages of the model on three text prompts, as seen below. (I used a random seed of 18,000.) We vary the number of inference steps of each stage of the model; increasing the number of inference steps increases the required computation.</p>

                <img src="media/a/part0_steps20.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=20</code>.</p>

                <p>For <code>num_inference_steps=20</code> (the recommended value), the first stage generates a reasonable 64x64 image for each of the three prompts, and the second stage does a great job of upscaling. The output images appear slightly more high-contrast than real life; for example, the neon-green portion of the rocket is slightly unrealistic.</p>

                <img src="media/a/part0_steps5.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=5</code>.</p>

                <p>For <code>num_inference_steps=5</code>, the 64x64 image for "a rocket ship" is nonsensical. Furthermore, the second stage does a poor job of upscaling the 64x64 image for "a man wearing a hat".</p>

                <img src="media/a/part0_steps40.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=40</code>.</p>

                <p>For <code>num_inference_steps=40</code>, the first and second stages perform well, though their tendency to generate high-contrast images is increased.</p>

            <h3>1.1 The Forward Process</h3>

                <p>A diffusion model works by iteratively removing noise from a noisy image until it arrives at a "clean" output image. The model starts with an image of pure noise or with a noisy version of a clean image. In the latter case, a "forward process" adds noise to the clean image before it is provided to the diffusion model.</p>

                <p>Given a clean image <code>x_0</code> and a timestep <code>t</code>, we compute the noisy image <code>x_t</code> as a linear combination of <code>x_0</code> and <code>epsilon</code>, where <code>epsilon</code> is a standard-normal image (in other words, an image of pure noise). For greater values of <code>t</code>, the contribution of <code>epsilon</code> is increased.</p>

                <p>We compute <code>x_t</code> for <code>t = [250, 500, 750]</code>. Our input <code>x_0</code> is a clean image of the Campanile.</p>

                <img src="media/a/part1-1.png" alt="img" width="auto" height="100">
                <p class="caption">The images <code>x_0</code>, <code>x_250</code>, <code>x_500</code>, and <code>x_750</code>.</p>

            <h3>1.2 Classical Denoising</h3>

                <p>A simple way to remove noise from an image is to blur it. We apply a Gaussian blur filter (with a kernel size of 13) to each <code>x_t</code>.</p>

                <img src="media/a/part1-2.png" alt="img" width="auto" height="300">
                <p class="caption">Top: The noisy image <code>x_250</code> and a blurred version of it.</p>
                <p class="caption">Middle: <code>x_500</code> and its blur.</p>
                <p class="caption">Bottom: <code>x_750</code> and its blur.</p>
            
            <h3>1.3 One-step Denoising</h3>

                <p>A better way to remove noise from an image is to pass it into a denoising UNet model. DeepFloyd provides a trained denoising UNet.</p>

                <img src="media/a/part1-3.png" alt="img" width="auto" height="300">
                <p class="caption">Top: The original Campanile image; with noise (<code>t=250</code>); denoised via UNet.</p>
                <p class="caption">Middle: Original; noisy (<code>t=500</code>); denoised.</p>
                <p class="caption">Bottom: Original; noisy (<code>t=750</code>); denoised.</p>
            
            <h3>1.4 Iterative Denoising</h3>

                <p>The UNet performs worse on highly noisy inputs, which makes sense. But a diffusion model can denoise an input with any amount of noise. In particular, DeepFloyd can denoise an image with any amount of noise from <code>t=0</code> (no noise) to <code>t=1000</code> (entirely noise).</p>

                <p>For a noisy image <code>x_t</code>, DeepFloyd provides a noise estimate <code>err</code> and a scalar constant <code>alpha_cumprod_t</code> that can be used to obtain <code>x_{t-1}</code>. Over hundreds of iterations, we can transform <code>x_t</code> into a clean image <code>x_0</code>.</p>

                <p>(To compute the estimated noise of <code>x_t</code>, DeepFloyd must receive an embedded text prompt that describes the desired <code>x_0</code>. By default, we supply the text prompt "a high quality photo".)</p>

                <p>Iterating hundreds of times is costly. We define a sequence of "strided timesteps" in which each strided timestep corresponds to 30 canonical timesteps. In particular, we have <code>st[0]=990, st[1]=960, ..., st[33]=0</code>. We use a modified denoising formula to iterate over strided timesteps instead of canonical timesteps; in each iteration, we compute <code>x_{t-30}</code> instead of <code>x_{t-1}</code>.</p>

                <p>We show the iterative denoising of a noisy image <code>x_690</code>.</p>

                <img src="media/a/part1-4_progression.png" alt="img" width="auto" height="100">
                <p class="caption">Selected intermediate results of denoising <code>x_690</code> with iterative denoising.</p>

                <img src="media/a/part1-4_compare.png" alt="img" width="auto" height="100">
                <p class="caption">For comparison: The results of iterative denoising, UNet denoising, and blurring on <code>x_690</code>.</p>

            <h3>1.5 Diffusion Model Sampling</h3>

                <p>We sample an image from our denoising process by passing in a pure-noise image with timestep <code>t=990</code>. (We also pass in the default text prompt.)</p>

                <img src="media/a/part1-5.png" alt="img" width="auto" height="100">
                <p class="caption">Five samples of "a high quality photo".</p>

            <h3>1.6 Diffusion Model Sampling</h3>

                <p>We improve the quality of our sampled images with "Classifier-Free Guidance". In addition to the default text prompt ("a high quality photo"), we pass in the null prompt ("") to DeepFloyd. For a given <code>x_t</code>, each of our prompts provides a noise estimate. The former prompt is a "condition prompt" that provides our conditioned estimate <code>err_c</code>; the latter prompt is an "unconditioned prompt" that provides our unconditioned estimate <code>err_u</code>.</p>

                <p>For some value <code>gamma &gt; 1</code>, we compute <code>err = err_u + gamma * (err_c - err_u)</code>. Note that <code>err</code> is essentially a weighted average in which <code>err_c</code> receives highly positive weight and <code>err_u</code> receives negative weight; informally, <code>err</code> is a more extreme version of <code>err_c</code>. We use <code>err</code> in our subsequent computation of <code>x_{t-30}.</code>

                <p>Now we sample images from our CFG-enhanced denoising process. We use the value <code>gamma = 7</code>.</p>

                <img src="media/a/part1-6.png" alt="img" width="auto" height="100">
                <p class="caption">Five CFG-enhanced samples of "a high quality photo".</p>

            <h3>1.7 Image-to-Image Translation</h3>

                <p>By providing the noisy version of a clean image to our denoising process, we obtain an output image that resembles the clean input with some random changes. For large <code>t</code> (e.g. <code>st[1]=960</code>), the output will have very little connection to the input; for small <code>t</code> (e.g. <code>st[20]=330</code>), the output will closely resemble the input.</p>

                <p>We use the Campanile image and two other real-life images as our clean inputs. For each image, we add noise to it according to a varying initial timestep and pass it into the denoising process.</p>

                <img src="media/a/part1-7_test.png" alt="img" width="auto" height="300">
                <p class="caption">Top: For <code>i = [1, 3, 5, 7, 10, 20]</code>, the result of image-to-image translation with initial timestep <code>st[i]</code> and the Campanile image as input.</p>
                <p class="caption">Middle: Image-to-image translations with <code>plushies.jpg</code>.</p>
                <p class="caption">Bottom: Image-to-image translations with <code>view.jpg</code>.</p>
                
            <h3>1.7.1 More Images</h3>

                <p>We also perform image-to-image translations with web images and drawn images.</p>

                <img src="media/a/part1-7_web.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Image-to-image translations with <a href="https://goodsmileshop.com/medias/sys_master/images/images/h6a/h56/9510882508830.jpg">Ky</a>, a character from the <i>Guilty Gear</i> franchise.</p>
                <p class="caption">Bottom: Image-to-image translations with <a href="https://fumo.website/img/001.jpg">Reimu</a>, a character from the <i>Touhou</i> franchise.</p>

                <img src="media/a/part1-7_drawn.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Image-to-image translations with a smiley face.</p>
                <p class="caption">Bottom: Image-to-image translations with a purple smiley face.</p>

            <h3>1.7.2 Inpainting</h3>

                <p>For our image of the Campanile and our two web images, we define rectangular "masks". We can modify the denoising process so that it produces outputs with changes in the masked areas only. Then we execute the denoising process (with initial timestep <code>st[0]=990</code>, in order to maximize the changes in the masked areas).</p>

                <img src="media/a/part1-7_inpainting.png" alt="img" width="auto" height="300">
                <p class="caption">Inpainting masks and results for the Campanile, Ky, and Reimu images.</p>

            <h3>1.7.3 Text-Conditioned Image-to-Image Translation</h3>
                
                <p>Observe that performing image-to-image translation with a high initial timestep yields a highly random output. We can reduce this randomness by replacing our generic condition prompt with a more specific condition prompt. (The unconditioned prompt stays the same.)</p>

                <img src="media/a/part1-7_condition.png" alt="img" width="auto" height="300">
                <p class="caption">Top: Image-to-image translations with the Campanile image. The condition prompt is "a rocket ship".</p>
                <p class="caption">Middle: Image-to-image translations with Ky. Prompt: "a photo of a dog".</p>
                <p class="caption">Bottom: Image-to-image translations with Reimu. Prompt: "a man with a hat".</p>

            <h3>1.8 Visual Anagrams</h3>

                <p>In this section and section 1.9, we produce visually interesting results by providing our denoising process with multiple condition prompts.</p>

                <p>A "visual anagram" depicts different things when viewed right-side-up and upside-down. To generate a visual anagram, we need a condition prompt for the right-side-up view and a condition prompt for the upside-down view.</p>

                <p>We execute our denoising process with a high initial timestep and an input of pure noise. For each (strided) iteration of the denoising process, we get a noise estimate <code>err_1</code> for the current <code>x_t</code> with our right-side-up prompt and a noise estimate <code>err_2</code> with our upside-down prompt. The weighted average <code>err = w * err_1 + (1 - w) * err_2</code> (for <code>0 &lt; w &lt; 1</code>) is used to compute <code>x_{t-30}</code>.</p> (Computing <code>err_2</code> requires us to flip <code>x_t</code> beforehand.)</p>

                <span>visual anagrams</span>

            <h3>1.9 Hybrid Images</h3>

                <p>A "hybrid image" depicts different things when viewed close-up and far-away. The far-away view relies on low-frequency features of the image, and the close-up view relies on high-frequency features. To generate a hybrid image, we need a condition prompt for its low-frequency features and a condition prompt for its high-frequency features.</p>

                <p>As with visual anagrams, we generate hybrid images by combining our noise estimates <code>err_1</code>, <code>err_2</code> for <code>x_t</code> in each (strided) iteration of the denoising process. For hybrid images, we have <code>err = low_freq(err_1) + high_freq(err_2)</code>, where <code>low_freq</code> is a Gaussian blur filter (with kernel size of 33, sigma of 2) and <code>high_freq</code> is its complement.</p>

                <span>hybrid images</span>

        <h2>Part B: Diffusion Models From Scratch</h2>

                

 </body>
 </html>