<!DOCTYPE html>
<html>

<head>
    <title>My Portfolio</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        
        img {
            /* From https://css-tricks.com/almanac/properties/i/image-rendering/ */
            image-rendering: auto;
            image-rendering: crisp-edges;
            image-rendering: pixelated;
            image-rendering: -webkit-optimize-contrast;

            /* Center all images */
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        .caption {
            font-size: 12px;
            text-align: center;
        }
    </style>
</head>

<body>

    <h1>Project 5: Diffusion Models</h1>

        <h2>Part A: Using Diffusion Models</h2>

                <p>Part A of this project relies on the DeepFloyd IF diffusion model, which is available on Hugging Face. The first stage of DeepFloyd IF takes a text prompt and outputs a 64x64 color image; the second stage upscales the image to 256x256.</p>

                <p>We call both stages of the model on three text prompts, as seen below. (I used a random seed of 18,000.) We vary the number of inference steps of each stage of the model; increasing the number of inference steps increases the required computation.</p>

                <img src="media/a/part0_steps20.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=20</code>.</p>

                <p>For <code>num_inference_steps=20</code> (the recommended value), the first stage generates a reasonable 64x64 image for each of the three prompts, and the second stage does a great job of upscaling. The output images appear slightly more high-contrast than real life; for example, the neon-green portion of the rocket is slightly unrealistic.</p>

                <img src="media/a/part0_steps5.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=5</code>.</p>

                <p>For <code>num_inference_steps=5</code>, the 64x64 image for "a rocket ship" is nonsensical. Furthermore, the second stage does a poor job of upscaling the 64x64 image for "a man wearing a hat".</p>

                <img src="media/a/part0_steps40.png" alt="img" width="auto" height="220">
                <p class="caption">Results for <code>num_inference_steps=40</code>.</p>

                <p>For <code>num_inference_steps=40</code>, the first and second stages perform well, though their tendency to generate high-contrast images is increased.</p>

            <h3>1.1 The Forward Process</h3>

                <p>A diffusion model works by iteratively removing noise from a noisy image until it arrives at a "clean" output image. The model starts with an image of pure noise or with a noisy version of a clean image. In the latter case, a "forward process" adds noise to the clean image before it is provided to the diffusion model.</p>

                <p>Given a clean image <code>x_0</code> and a timestep <code>t</code>, we compute the noisy image <code>x_t</code> as a linear combination of <code>x_0</code> and <code>epsilon</code>, where <code>epsilon</code> is an image of pure noise. For greater values of <code>t</code>, the contribution of <code>epsilon</code> is increased.</p>

                <p>We compute <code>x_t</code> for <code>t = [250, 500, 750]</code>. Our input <code>x_0</code> is a clean image of the Campanile.</p>

                <img src="media/a/part1-1.png" alt="img" width="auto" height="100">
                <p class="caption">The images <code>x_0</code>, <code>x_250</code>, <code>x_500</code>, and <code>x_750</code>.</p>

            <h3>1.2 Classical Denoising</h3>

                <p>A simple way to remove noise from an image is to blur it. We apply a Gaussian blur filter (with a kernel size of 13) to each <code>x_t</code>.</p>

                <img src="media/a/part1-2.png" alt="img" width="auto" height="300">
                <p class="caption">Top: The noisy image <code>x_250</code> and a blurred version of it.</p>
                <p class="caption">Middle: <code>x_500</code> and its blur.</p>
                <p class="caption">Bottom: <code>x_750</code> and its blur.</p>
            
            <h3>1.3 One-step Denoising</h3>

                <p>A better way to remove noise from an image is to pass it into a denoising UNet model. We use the trained UNet provided by DeepFloyd.</p>

                <img src="media/a/part1-3.png" alt="img" width="auto" height="300">
                <p class="caption">Top: The original Campanile image; with noise (<code>t=250</code>); denoised via UNet.</p>
                <p class="caption">Middle: Original; noisy (<code>t=500</code>); denoised.</p>
                <p class="caption">Bottom: Original; noisy (<code>t=750</code>); denoised.</p>
            
            <h3>1.4 Iterative Denoising</h3>

                <p>The UNet performs worse on highly noisy inputs, which makes sense. But a diffusion model can denoise an input with any amount of noise. In particular, DeepFloyd can denoise an image with any amount of noise from <code>t=0</code> (no noise) to <code>t=1000</code> (entirely noise).</p>

                <p>For a noisy image <code>x_t</code>, DeepFloyd provides a noise estimate <code>err</code> and a scalar constant <code>alpha_cumprod_t</code> that can be used to obtain <code>x_{t-1}</code>. Over hundreds of iterations, we can transform <code>x_t</code> into a clean image <code>x_0</code>.</p>

                <p>(To compute the estimated noise of <code>x_t</code>, DeepFloyd must receive an embedded text prompt that describes the desired <code>x_0</code>. By default, we supply the text prompt "a high quality photo".)</p>

                <p>Iterating hundreds of times is costly. We define a sequence of "strided timesteps" in which each strided timestep corresponds to 30 canonical timesteps. In particular, we have <code>st[0]=990, st[1]=960, ..., st[33]=0</code>. We use a modified denoising formula to iterate over strided timesteps instead of canonical timesteps; in each iteration, we compute <code>x_{t-30}</code> instead of <code>x_{t-1}</code>.</p>

                <p>We show the iterative denoising of a noisy image <code>x_690</code>.</p>

                <img src="media/a/part1-4_progression.png" alt="img" width="auto" height="100">
                <p class="caption">Selected intermediate results of denoising <code>x_690</code> with iterative denoising.</p>

                <img src="media/a/part1-4_compare.png" alt="img" width="auto" height="100">
                <p class="caption">For comparison: The results of iterative denoising, UNet denoising, and blurring on <code>x_690</code>.</p>

            <h3>1.5 Diffusion Model Sampling</h3>

                <p>We sample an image from our denoising process by passing in a pure-noise image with timestep <code>t=990</code>. (We also pass in the default text prompt.)</p>

                <img src="media/a/part1-5.png" alt="img" width="auto" height="100">
                <p class="caption">Five samples of "a high quality photo".</p>

            <h3>1.6 Diffusion Model Sampling</h3>

                <p>We improve the quality of our sampled images with "Classifier-Free Guidance". In addition to the default text prompt ("a high quality photo"), we pass in the null prompt ("") to DeepFloyd. For a given <code>x_t</code>, each of our prompts provides a noise estimate. The former prompt is a "condition prompt" that provides our conditioned estimate <code>err_c</code>; the latter prompt is an "unconditioned prompt" that provides our unconditioned estimate <code>err_u</code>.</p>

                <p>For some value <code>gamma &gt; 1</code>, we compute <code>err = err_u + gamma * (err_c - err_u)</code>. Note that <code>err</code> is essentially a weighted average in which <code>err_c</code> receives highly positive weight and <code>err_u</code> receives negative weight; informally, <code>err</code> is a more extreme version of <code>err_c</code>. We use <code>err</code> in our subsequent computation of <code>x_{t-30}.</code></p>

                <p>Now we sample images from our CFG-enhanced denoising process. We use the value <code>gamma = 7</code>.</p>

                <img src="media/a/part1-6.png" alt="img" width="auto" height="100">
                <p class="caption">Five CFG-enhanced samples of "a high quality photo".</p>

            <h3>1.7 Image-to-Image Translation</h3>

                <p>By providing the noisy version of a clean image to our denoising process, we obtain an output image that resembles the clean input with some random changes. For large <code>t</code> (e.g. <code>st[1]=960</code>), the output will have very little connection to the input; for small <code>t</code> (e.g. <code>st[20]=330</code>), the output will closely resemble the input.</p>

                <p>We use the Campanile image and two other real-life images as our clean inputs. For each image, we add noise to it according to a varying initial timestep and pass it into the denoising process.</p>

                <img src="media/a/part1-7_test.png" alt="img" width="auto" height="300">
                <p class="caption">Top: For <code>i = [1, 3, 5, 7, 10, 20]</code>, the result of image-to-image translation with initial timestep <code>st[i]</code> and the Campanile image as input.</p>
                <p class="caption">Middle: Image-to-image translations with <code>plushies.jpg</code>.</p>
                <p class="caption">Bottom: Image-to-image translations with <code>view.jpg</code>.</p>
                
            <h3>1.7.1 More Images</h3>

                <p>We also perform image-to-image translations with web images and drawn images.</p>

                <img src="media/a/part1-7_web.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Image-to-image translations with <a href="https://goodsmileshop.com/medias/sys_master/images/images/h6a/h56/9510882508830.jpg">Ky</a>, a character from the <i>Guilty Gear</i> franchise.</p>
                <p class="caption">Bottom: Image-to-image translations with <a href="https://fumo.website/img/001.jpg">Reimu</a>, a character from the <i>Touhou</i> franchise.</p>

                <img src="media/a/part1-7_drawn.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Image-to-image translations with a smiley face.</p>
                <p class="caption">Bottom: Image-to-image translations with a purple smiley face.</p>

            <h3>1.7.2 Inpainting</h3>

                <p>For our image of the Campanile and our two web images, we define rectangular "masks". We can modify the denoising process so that it produces outputs with changes in the masked areas only. Then we execute the denoising process. We choose an initial timestep of <code>st[0]=990</code> in order to maximize the changes in the masked areas.</p>

                <img src="media/a/part1-7_inpainting.png" alt="img" width="auto" height="300">
                <p class="caption">Inpainting masks and results for the Campanile, Ky, and Reimu images.</p>

            <h3>1.7.3 Text-Conditioned Image-to-Image Translation</h3>
                
                <p>Observe that performing image-to-image translation with a high initial timestep yields a highly random output. We can reduce this randomness by replacing our generic condition prompt with a more specific condition prompt. The unconditioned prompt stays the same.</p>

                <img src="media/a/part1-7_condition.png" alt="img" width="auto" height="300">
                <p class="caption">Top: Image-to-image translations with the Campanile image. The condition prompt is "a rocket ship".</p>
                <p class="caption">Middle: Image-to-image translations with Ky. Prompt: "a photo of a dog".</p>
                <p class="caption">Bottom: Image-to-image translations with Reimu. Prompt: "a man with a hat".</p>

            <h3>1.8 Visual Anagrams</h3>

                <p>In this section and section 1.9, we produce visually interesting results by providing our denoising process with multiple condition prompts.</p>

                <p>A "visual anagram" depicts different things when viewed right-side-up and upside-down. To generate a visual anagram, we need a condition prompt for the right-side-up view and a condition prompt for the upside-down view.</p>

                <p>We execute our denoising process with a high initial timestep and an input of pure noise. For each (strided) iteration of the denoising process, we get a noise estimate <code>err_1</code> for the current <code>x_t</code> with our right-side-up prompt and a noise estimate <code>err_2</code> with our upside-down prompt. The weighted average <code>err = w * err_1 + (1 - w) * err_2</code> (for <code>0 &lt; w &lt; 1</code>) is used to compute <code>x_{t-30}</code>. Note that computing <code>err_2</code> requires us to flip <code>x_t</code> beforehand. The weight <code>w</code> is determined by trial and error.</p>
                
                <img src="media/a/part1-8_campfire.png" alt="img" width="auto" height="100">
                <br>
                <img src="media/a/part1-8_man.png" alt="img" width="auto" height="100">
                <p class="caption">A visual anagram with right-side-up prompt "an oil painting of people around a campfire", upside-down prompt "an oil painting of an old man", and weight <code>w = 0.7</code>.</p>

                <img src="media/a/part1-8_chocolate.png" alt="img" width="auto" height="100">
                <br>
                <img src="media/a/part1-8_dog.png" alt="img" width="auto" height="100">
                <p class="caption">Prompts: "an oil painting of chocolate pieces", "an oil painting of a brown dog". Weight: <code>w = 0.7</code>.</p>

                <img src="media/a/part1-8_snow.png" alt="img" width="auto" height="100">
                <br>
                <img src="media/a/part1-8_coast.png" alt="img" width="auto" height="100">
                <p class="caption">Prompts: "an oil painting of a snowy mountain village", "a photo of the amalfi cost" [sic]. Weight: <code>w = 0.75</code>.</p>
                

            <h3>1.9 Hybrid Images</h3>

                <p>A "hybrid image" depicts different things when viewed far-away and close-up. The far-away view relies on low-frequency features of the image, and the close-up view relies on high-frequency features. To generate a hybrid image, we need a condition prompt for its low-frequency features and a condition prompt for its high-frequency features.</p>

                <p>As with visual anagrams, we generate hybrid images by combining our noise estimates <code>err_1</code>, <code>err_2</code> for <code>x_t</code> in each (strided) iteration of the denoising process. For hybrid images, we have <code>err = low_freq(err_1) + high_freq(err_2)</code>, where <code>low_freq</code> is a Gaussian blur filter (with kernel size of 33, sigma of 2) and <code>high_freq</code> is its complement.</p>

                <img src="media/a/part1-9_waterfall1.png" alt="img" width="auto" height="100">
                <p class="caption">A hybrid image with far-away prompt "a lithograph of a skull" and close-up prompt ""a lithograph of waterfalls".</p>

                <img src="media/a/part1-9_rocket1.png" alt="img" width="auto" height="100">
                <p class="caption">Far-away: "a pencil". Close-up: "a rocket ship".</p>

                <img src="media/a/part1-9_fish1.png" alt="img" width="auto" height="100">
                <p class="caption">Far-away: "a lithograph of planet earth". Close-up: "a lithograph of blue and green fishes".</p>

        <h2>Part B: Diffusion Models From Scratch</h2>

                <p>In the second part of this project, we create a diffusion model that generates images which resemble digits in the <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html">MNIST</a> database.</p>

            <h3>Part 1: Single-Step Denoising via UNet</h3>

                <p>Our overall goal is to generate MNIST-like images from images of pure noise. An intermediate step is to train a UNet that cleans up noisy versions of MNIST images.</p>

                <p>We implement a UNet with the following architecture, as shown in the project spec. Unlike a diffusion model, which removes noise from an image over the course of many iterations, the UNet is designed to denoise an image in a single iteration.</p>

                <img src="media/b/part1_unet.png" alt="img" width="auto" height="300">

                <p>To obtain training data for the UNet, we modify each datapoint in the training portion of the MNIST dataset. Each MNIST datapoint consists of a 28x28 image of a handwritten digit and a label indicating its value.</p>

                <p>We add noise to a clean MNIST image according to the following formula: <code>z = x + delta * epsilon</code>, where <code>x</code> is the clean image, <code>delta</code> is a scalar constant, and <code>epsilon</code> is standard-normal noise. Each <code>(z, x)</code> pair serves as a training datapoint for the UNet: given <code>z</code>, the UNet should recover <code>x</code>. For now, we ignore the original labels of the MNIST dataset.</p>

                <p>Our noising process produces the following results for various values of <code>delta</code>:</p>

                <img src="media/b/part1_noising.png" alt="img" width="auto" height="400">

                <p>In practice, all of our training datapoints have <code>delta=0.5</code> noise, under the assumption that a thoroughly-trained UNet will be able to denoise images with other amounts of noise.</p>

                <p>Our modified training data contains 60,000 datapoints. We train the UNet over five epochs with a batch size of 256, a hidden dimension of <code>D = 128</code>, and an Adam optimizer with learning rate <code>1e-4</code>. Our loss function computes the squared error between each outputted image and its corresponding true clean image.</p>

                <p>We get the following training losses:</p>

                <img src="media/b/part1_training_losses_UC.png" alt="img" width="auto" height="400">

                <p>We generate test datapoints in the same way we generated training datapoints, except we now derive our datapoints from the test portion of MNIST. We test the UNet after its first and fifth epochs.</p>

                <img src="media/b/part1_epoch01.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Original MNIST images. Middle: With <code>delta=0.5</code> noise. Bottom: Denoised by first-epoch UNet.</p>


                <img src="media/b/part1_epoch05.png" alt="img" width="auto" height="200">
                <p class="caption">Top: Original MNIST images. Middle: With <code>delta=0.5</code> noise. Bottom: Denoised by fifth-epoch UNet.</p>

                <p>We can see that the fifth-epoch UNet produces fewer visual artefacts than the first-epoch UNet.</p>

                <p>Once the UNet has finished training, we perform "out-of-distribution" testing. We generate noisy images using various values of <code>delta</code> and inspect the results of denoising with the UNet. (It is important that we derive the noisy images from the test portion, not the training portion, of the MNIST dataset.)</p>

                <img src="media/b/part1_outofdistribution.png" alt="img" width="auto" height="200">

                <p class="caption">Top: A MNIST test image noised with <code>delta = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</code>.</p>
                <p class="caption">Bottom: The results of denoising each noised image with the fully-trained UNet.</p>

            <h3>Part 2: Time-Conditional and Class-Conditional Diffusion Models</h3>

                <p>Unlike the UNet, our desired diffusion model must denoise iteratively and must be able to denoise an image according to some input timestep. We modify the original UNet architecture to support a scalar input <code>t</code> that accompanies each input image.</p>

                <img src="media/b/part2_unet.png" alt="img" width="auto" height="300">

                <p>For training and testing our diffusion model, we employ the "forwarding" and "sampling" approach of <a href="https://arxiv.org/abs/2006.11239">DDPM</a> models. Under this approach, our "time-conditional" UNet no longer tries to estimate the clean version of an input noisy image; instead, it tries to estimate the error of the noisy image. This is a minor change, but it is helpful during forwarding.</p>

                <p>To train the time-conditional UNet, we first precompute the DDPM values <code>(alpha_t, beta_t, alpha_cumprod_t)</code> for each timestep <code>0 &le; t &le; 300</code>. Then we repeatedly select random clean images <code>x_0</code> from the training portion of MNIST, random standard-normal errors <code>epsilon</code>, and random timesteps <code>1 &le; t &le; 300</code>. For each <code>(x_0, epsilon, t)</code> triplet, we compute a noisy image <code>x_t</code>. Passing <code>(x_t, t)</code> into the time-conditional UNet yields an estimate for <code>epsilon</code>, which is compared to the true value. (This repeated process is the "forwarding" of a DDPM model.)</p>

                <p>We train over twenty epochs of the training portion of MNIST. We use a batch size of 128, a hidden dimension of <code>D = 64</code>, and an Adam optimizer with an initial learning rate of <code>1e-3</code> that decays exponentially to <code>1e-4</code>.</p>

                <p>We get the following training losses for the time-conditional UNet:</p>

                <img src="media/b/part2_training_losses_TC.png" alt="img" width="auto" height="400">

                <p>During and after the training of the time-conditional UNet, we test it (in other words, we perform "sampling") by iteratively removing noise from a pure-noise image. Each call to the time-conditional UNet constitutes a single timestep, so we need 300 calls to arrive at a clean image. Remarkably, the test portion of MNIST is completely unnecessary.</p>
                    
                <p>We test the following epochs of the time-conditional UNet:</p>

                <img src="media/b/timecond_epoch1.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 1</p>
                <img src="media/b/timecond_epoch5.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 5</p>
                <img src="media/b/timecond_epoch10.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 10</p>
                <img src="media/b/timecond_epoch15.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 15</p>
                <img src="media/b/timecond_epoch20.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 20</p>

                <p>The test results of the time-conditional UNet have many artefacts and do not always resemble actual digits. We improve on our time-conditional UNet by modifying its architecture to support a third parameter <code>c</code>, a one-hot encoding that represents a particular digit from 0 to 9. The value for <code>c</code> enters the "class-conditional" UNet at the same modules as <code>t</code>.</p>

                During training, we pass in the encoded label of clean image <code>x_0</code> as the value for <code>c</code>. (To ensure the class-conditional UNet supports arbitrary generation in addition to class-specific generation, we randomly "zero out" 10% of the encodings.) Otherwise, training the class-conditional UNet is identical to training the time-conditional UNet.

                <p>We get the following training losses for the class-conditional UNet:</p>

                <img src="media/b/part2_training_losses_CC.png" alt="img" width="auto" height="400">

                <p>Testing the class-conditional UNet is likewise similar to testing the time-conditional UNet. In our testing, we pass in specific values for <code>c</code> to verify that the class-conditional UNet supports class-specific generation.</p>

                <p>We test the following epochs of the class-conditional UNet:</p>

                <img src="media/b/classcond_epoch1.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 1</p>
                <img src="media/b/classcond_epoch5.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 5</p>
                <img src="media/b/classcond_epoch10.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 10</p>
                <img src="media/b/classcond_epoch15.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 15</p>
                <img src="media/b/classcond_epoch20.gif" alt="img" width="auto" height="150">
                <p class="caption">Epoch 20</p>

                <p>We can see that the class-conditional UNet generates images of far better quality than the time-conditional UNet.</p>

 </body>
 </html>